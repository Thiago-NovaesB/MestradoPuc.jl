{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e60f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad6df984",
   "metadata": {},
   "source": [
    "## Problema 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cb5ba2",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    &\\min && \\Sigma_{ij \\in A} c_{ij}x_{ij}  \\\\\n",
    "    & st && \\Sigma_j x_{ij} - \\Sigma_j x_{ji} = 1 & i=s\\\\\n",
    "        &&& \\Sigma_j x_{ij} - \\Sigma_j x_{ji} = -1& i=t\\\\\n",
    "        &&& \\Sigma_j x_{ij} - \\Sigma_j x_{ji} = 0 & otherwise\\\\\n",
    "        &&& x_{ij} \\in \\{ 0,1\\} & \\forall ij \\in A\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af3bae9",
   "metadata": {},
   "source": [
    "$ij$ representa a aresta de conexão dos vértice $i$ para vértice $j$. $A$ é o conjunto de arrestas. $s$ é o ponto de partica e $t$ o ponto de chegada. $x_{ij}$ é 1 se a aresta foi usada e 0 caso contrario. Sendo assim, a primeira restrição indica que o vértice de partida deve ter uma saida a mais que entradas. A segunda restrição indica que o vértice de chegada deve ter uma saida a menos que entradas. A terceira restrição indica que os outros vértices devem ter o mesmo numero de saidas e entradas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac4c55a",
   "metadata": {},
   "source": [
    "A matriz do problema possui $n$ linhas e $n^2$ colunas, onde $n$ é o numero de vértices. Analisando as restrições é facil ver que as colunas são nulas ou possuem exatamente um numero $1$ e um numero $-1$. Usando esses fator vamos mostrar que ela é totalmente unimodular.\n",
    "Lembrando que queremos mostrar que qualquer submatrix quadrada não tem inversa ou é unimodular. Como os coeficiente da matriz são 0, 1 ou -1, basta mostrar que as submatrizes tem determinante 0 (não tem inversa), 1 (unimodular) ou -1 (unimodular)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee65dd52",
   "metadata": {},
   "source": [
    "Relembrando a definição de determinante. Seja um funcional linear em matrizes quadradas com as seguintes propriedades:\n",
    "<ol>\n",
    "  <li>$F(x_1, ... ,x_i, ...,x_j, ...,x_n) = -F(x_1, ... ,x_j, ...,x_i, ...,x_n)$ Troca linhas ou colunas muda o sinal</li>\n",
    "  <li>$F(x_1, ... ,kx_i, ...,x_n) = kF(x_1, ..., x_i, ...,x_n)$ Multiplicar uma linha ou coluna por um numero, multiplica o funcional</li>\n",
    "  <li>$F(x_1, ... ,x_i, ...,x_j, ...,x_n) = F(x_1, ... ,x_i + kx_j, ...,x_j, ...,x_n)$ Somar multiplos de uma linha ou coluna, respectivamente, em outra linha ou coluna, não altera o funcional</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64fb2d",
   "metadata": {},
   "source": [
    "Esse funcional é unico e chamamos ele de determinante (Algebra Linear - Elon Lages, IMPA.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023708fc",
   "metadata": {},
   "source": [
    "Com essas propriedades, podemos construir um algoritmo para calcular o determinante:\n",
    "<ol>\n",
    "  <li>Escalone a matriz até ficar triangular, anotando as operações</li>\n",
    "  <li>O determinante da matriz triangular é o produto da diagonal principal</li>\n",
    "  <li>Usando as anotações das operações durante o escalonamento e as propriedades do determinante, recupere o determinante da matriz original</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f27d9",
   "metadata": {},
   "source": [
    "A matriz de nosso interesse so possui 1 e -1 para serem eliminados no escalonamento. Como cada coluna so possui no maximo um 1 e um -1, ao somar uma linha na outra, teremos as operações 0+0, 1+0, -1+0 e -1+1. Todas essas operações produzem 0, 1 ou -1. Ou seja, podemos escalonar a matriz apenas somando linhas umas nas outras (não altera o determinante) ou trocar linhas de lugar (muda o sinal do determinante), sem criar nenhum numero diferente de 0,1 e -1. Ao final do escalonamento, a diagonal principal contera apenas 0, 1 e -1. Ou seja o determinante sera 0, 1 ou -1. No maximo o sinal devera ser trocado (devido a trocas de linhas durante o escalonamento), mas permanecera como 0, 1 ou -1. Logo a matriz do problema é totalmente unimodular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a987bb4b",
   "metadata": {},
   "source": [
    "Em relação ao algoritmo, como as matriz é totalmente unimodular, todas as bases usadas ao longo do simplex são unimodulares, ou seja, vão produzir uma solução inteira (pois o RHS é inteiro). Logo podemos relaxar as restrições de integralidade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197014b",
   "metadata": {},
   "source": [
    "## Problema 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99252d60",
   "metadata": {},
   "source": [
    "Assumindo que problemas de maximização inviáveis tem valor ótimo menos infinito e problemas de maximização ilimitados tem valor ótimo infinito (Usando reta real extendida). Quando o primeiro problema é inviável, $z$ é menos infinito, então teremos $z(u) \\geq z$. Em outros casos, perceba que para todo $x$ viável no primeiro problema temos $z \\geq c^Tx$. Como o segundo problema tem menos restrições, para todo $x$ viável no primeiro problema ele também é viável no segundo problema. Temos que $z(u) \\geq c^Tx + u^T(b-Ax)$. Como estamos supondo que $x$ é viável, então $b-Ax$ tem todas as coordenadas não negativas, como por hipótese $u$ tem todas as coordenadas não negativas, $u^T(b-Ax)$ é não negativo. Sendo assim, se o primeiro problema é ilimitado, o segundo também é e vale $z(u) \\geq z$. Se o primeiro problema tem solução ótimo $x^*$, então temos $z(u) \\geq c^Tx^* + u^T(b-Ax^*) \\geq c^Tx^* = z$.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132136ff",
   "metadata": {},
   "source": [
    "## Problema 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee7df44",
   "metadata": {},
   "source": [
    "Perceba que (D) é um problema de programação linear e que seu dual, chamado aqui de P, é $max\\{c^Tx | Ax \\leq b x \\in \\mathbb{R}^n_+\\}$. Perceba também que (P) é uma relaxação de (PI), uma vez que a única diferença entre eles é que (P) possui variáveis reais e que (PI) possui variáveis inteiras. Por dualidade forte da programação linear, temos que $z_D = z_P$. Por ser relaxação, temos que $z_P \\geq z_{PI}$, então $z_D = z_P \\geq z_{PI}$. Além disso como (D) é um problema de minimização, se ele é ilimitado, $z_D = - \\infty$, então $z_{PI} = - \\infty$, mas como (PI) é um problema de maximização, isso significa ser inviável. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3997be",
   "metadata": {},
   "source": [
    "## Problema 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec10e867",
   "metadata": {},
   "source": [
    "A única diferença entrer $P_1$ e $P_2$ está nas restrições, sendo assim, basta mostrar que o conjunto viável de $P_1$ está contido em $P_2$. Mais precisamente, a diferença é que em $P_1$ temos $Ax=b$ e em $P_2$ temos $u^TAx=u^Tb$, que é equivalente à $u^T(Ax-b)=0$, que claramente é verdade quando $Ax=b$. Sendo assim, $P_2$ é relaxação de $P_1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988bbf52",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
